<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="强化方法," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2" />






<meta name="description" content="本文主要介绍强化学习中涉及的蒙特卡洛方法，该方法是强化学习求解免模型（model-free）方法中的重要组成部分，主要分为同策略（on-policy）和异策略（off-policy）两个分支。另外，在具体求解Q函数的时候，还分为first-visit和all-visit两种计算技巧。">
<meta name="keywords" content="强化方法">
<meta property="og:type" content="article">
<meta property="og:title" content="强化学习之蒙特卡洛方法">
<meta property="og:url" content="http://yoursite.com/2017/10/17/强化学习之蒙特卡洛方法/index.html">
<meta property="og:site_name" content="野草集">
<meta property="og:description" content="本文主要介绍强化学习中涉及的蒙特卡洛方法，该方法是强化学习求解免模型（model-free）方法中的重要组成部分，主要分为同策略（on-policy）和异策略（off-policy）两个分支。另外，在具体求解Q函数的时候，还分为first-visit和all-visit两种计算技巧。">
<meta property="og:image" content="http://orjn2q9xs.bkt.clouddn.com/On-Policy%20MC.png">
<meta property="og:image" content="http://orjn2q9xs.bkt.clouddn.com/Off-Policy%20MC.png">
<meta property="og:updated_time" content="2017-10-17T11:21:55.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="强化学习之蒙特卡洛方法">
<meta name="twitter:description" content="本文主要介绍强化学习中涉及的蒙特卡洛方法，该方法是强化学习求解免模型（model-free）方法中的重要组成部分，主要分为同策略（on-policy）和异策略（off-policy）两个分支。另外，在具体求解Q函数的时候，还分为first-visit和all-visit两种计算技巧。">
<meta name="twitter:image" content="http://orjn2q9xs.bkt.clouddn.com/On-Policy%20MC.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2017/10/17/强化学习之蒙特卡洛方法/"/>





  <title>强化学习之蒙特卡洛方法 | 野草集</title>
  














</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">野草集</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/10/17/强化学习之蒙特卡洛方法/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="左">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/1.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="野草集">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">强化学习之蒙特卡洛方法</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-10-17T19:07:34+08:00">
                2017-10-17
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv">本文总阅读量
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>次
            </span>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <blockquote>
<p> 本文主要介绍强化学习中涉及的蒙特卡洛方法，该方法是强化学习求解免模型（model-free）方法中的重要组成部分，主要分为同策略（on-policy）和异策略（off-policy）两个分支。另外，在具体求解Q函数的时候，还分为first-visit和all-visit两种计算技巧。</p>
</blockquote>
<a id="more"></a>
<h2 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h2><ol>
<li><strong>什么是蒙特卡洛方法？</strong>简单来说，蒙特卡洛方法就是利用统计学的思想来对Q函数进行求解，采用<strong>平均累积奖赏作为期望累积奖赏的近似</strong>。这样一来，就避免了大量的数值计算过程，并且还能够解决环境模型未知的情况。</li>
<li>蒙特卡洛方法同样具有其局限性，它只能够解决有限状态下的强化学习任务，即每一次episode都有一个终止状态。</li>
<li>值得注意的是，蒙特卡洛方法<strong>只涉及到Q函数的求解</strong>，并不涉及V函数的求解。这是因为通过Q函数我们可以很快得到最优策略$\pi$，而又因为此时环境未知，无法从V函数快速得到Q函数，所以此时对V函数求解意义不大。</li>
</ol>
<h2 id="方法描述"><a href="#方法描述" class="headerlink" title="方法描述"></a>方法描述</h2><p>在模型未知的情况下，从初始状态出发，使用某种策略（通常初始策略是$\pi(a|s)=\frac{1}{A(s)}$）进行采样，执行该策略获得轨迹</p>
<script type="math/tex; mode=display">
<x_0, a_0, r_1, x_1, a_1, r_2, \ldots, x_{T-1}, a_{T-1}, r_T, x_T></script><p>然后，对轨迹中出现的每一对状态-动作，记录其后的奖赏之和，作为该状态-动作对的一次累积奖赏采样值。多次采样得到多条轨迹后，将每个状态-动作对的累积奖赏值进行平均，即得到状态-动作值函数（Q函数）的估计。</p>
<h2 id="同策略方法"><a href="#同策略方法" class="headerlink" title="同策略方法"></a>同策略方法</h2><p>可以看出，要想得到Q函数的良好近似，就需要多次采样以得到不同的轨迹，以期遍历所有的<strong>状态空间和动作空间</strong>。同时，在实际操作中，可能我们每次迭代得到的策略是确定性的，即$\pi(a|s)=1$，或$\pi=arg \max_aQ(s,a)$，这样带来的后果就是我们无法遍历所有的状态空间和动作空间，面临“仅利用”的困境。为了跳出这一困境，就需要制定一种新的策略，使得所有动作都有可能被选中，这种新策略的数学定义如下：</p>
<script type="math/tex; mode=display">
\pi^{\varepsilon}(x)=
\begin{cases}
    \pi(s) &\text{以概率}1-\varepsilon \\
    \text{A中以均匀概率选取的动作} &\text{以概率}\varepsilon
\end{cases}</script><p>其中，$\pi(s)$被称为<strong>原始策略</strong>，即算法每次迭代后得出的策略，也是最后所要求的策略，$\pi^{\varepsilon}(s)$被称为<strong>$\varepsilon$贪心策略</strong>。从新策略的数学定义可以看出，当前最优动作被选中的概率是$1-\varepsilon+\frac{\varepsilon}{|A|}$，而每个非最优的动作被选中的概率是$\frac{\varepsilon}{|A|}$。 </p>
<p><img src="http://orjn2q9xs.bkt.clouddn.com/On-Policy%20MC.png" alt="通策略蒙特卡洛算法"></p>
<p>同策略蒙特卡洛学习算法生成的是$\varepsilon$贪心策略，而我们要求的是原始策略，能否在策略评估的时候使用$\varepsilon$贪心策略，而在策略改进的使用使用原始策略呢？答案是肯定的，这就导致了异策略算法的发明。</p>
<h2 id="异策略方法"><a href="#异策略方法" class="headerlink" title="异策略方法"></a>异策略方法</h2><h3 id="重要性采样"><a href="#重要性采样" class="headerlink" title="重要性采样"></a>重要性采样</h3><p>函数$f(x)$在概率分布$p$下的期望可表达为</p>
<script type="math/tex; mode=display">
\mathbb{E}[f]=\int_x p(x)f(x)dx</script><p>可通过从概率分布$p$上的采样${x_1,x_2,\ldots,x_m}$来估计$f$的期望，即</p>
<script type="math/tex; mode=display">
\hat{\mathbb{E}}[f]=\frac{1}{m} \sum_{i=1}^{m} f(x_i)</script><p>这时候引入另一个分布$q$，则函数$f$在概率分布$p$下的期望也可等价地写为</p>
<script type="math/tex; mode=display">
\mathbb{E}[f]=\int_x q(x)\frac{p(x)}{q(x)}f(x)dx</script><p>通过在$q$上的采样${x_1^{‘}, x_2^{‘}, \ldots, x_m^{‘}}$可估计$f$的期望如下：</p>
<script type="math/tex; mode=display">
\hat{\mathbb{E}}[f]=\frac{1}{m} \sum_{i=1}^{m} \frac{p(x_i^{'})}{q(x_i^{'})} f(x_i^{'})</script><p>这样的基于一个分布的采样来估计另一个分布下的期望称为<strong>重要性采样</strong>。</p>
<h3 id="算法推导"><a href="#算法推导" class="headerlink" title="算法推导"></a>算法推导</h3><p>使用策略$\pi$的采样轨迹来评估策略$\pi$，实际上就是用累积奖赏估计期望</p>
<script type="math/tex; mode=display">
Q(x,a)=\frac{1}{m} \sum_{i=1}^{m}R_i</script><p>由上述的重要性采样定理可知，我们可以通过策略$\pi^{‘}$的采样轨迹来评估策略$\pi$，则仅需对累积奖赏加权</p>
<script type="math/tex; mode=display">
Q(x,a)=\frac{1}{m} \sum_{i=1}^{m} \frac{P^{\pi}_i}{P^{\pi^{'}}_i} R_i</script><p>其中，$P^{\pi}_i$和$P^{\pi^{‘}}_i$分别表示两个策略产生第$i$条轨迹的概率。对于一条给定的轨迹$<x_0,a_0,r_1,\ldots, x_{t-1},="" a_{t-1},="" r_t,="" x_t="">$，策略$\pi$产生该轨迹的概率为</x_0,a_0,r_1,\ldots,></p>
<script type="math/tex; mode=display">
P^{\pi}=\prod_{i=0}^{T-1} \pi(x_i,a_i)P^{a_i}_{x_i \rightarrow x_{i+1}}</script><p>由此可以计算得出$P^{\pi}_i$和$P^{\pi^{‘}}_i$的比值为</p>
<script type="math/tex; mode=display">
\frac{P^{\pi}}{P^{\pi^{'}}}=\prod_{i=0}^{T-1}\frac{\pi(x_i,a_i)}{\pi^{'}(x_i,a_i)}</script><p>异策略蒙特卡洛算法就是在策略评估的时候使用的是$\varepsilon$贪心策略，而在策略改进的时候使用的是原始策略，这样最后算法得到的就是原始策略，符合我们的要求。其算法流程如下所示：</p>
<p><img src="http://orjn2q9xs.bkt.clouddn.com/Off-Policy%20MC.png" alt="异策略蒙特卡洛学习"></p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/强化方法/" rel="tag"># 强化方法</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/10/16/强化学习之策略迭代方法和值迭代方法/" rel="next" title="强化学习之策略迭代方法和值迭代方法">
                <i class="fa fa-chevron-left"></i> 强化学习之策略迭代方法和值迭代方法
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/1.jpg"
               alt="左" />
          <p class="site-author-name" itemprop="name">左</p>
           
              <p class="site-description motion-element" itemprop="description">那就热爱生活吧</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives/">
                <span class="site-state-item-count">6</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">3</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#背景介绍"><span class="nav-number">1.</span> <span class="nav-text">背景介绍</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#方法描述"><span class="nav-number">2.</span> <span class="nav-text">方法描述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#同策略方法"><span class="nav-number">3.</span> <span class="nav-text">同策略方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#异策略方法"><span class="nav-number">4.</span> <span class="nav-text">异策略方法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#重要性采样"><span class="nav-number">4.1.</span> <span class="nav-text">重要性采样</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#算法推导"><span class="nav-number">4.2.</span> <span class="nav-text">算法推导</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">左</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Gemini
  </a>
</div>


        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      本站访客数
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      人次
    </span>
  

  
    <span class="site-pv">
      本站总访问量
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      次
    </span>
  
</div>


        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.2"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script>



  


  




	





  





  






  





  

  

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
