<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="强化学习," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.2" />






<meta name="description" content="本文主要讲解了马尔科夫决策过程的数学描述以及数学推导，并且为了方便理解马尔科夫决策过程，还添加了对马尔科夫性以及马尔科夫奖励过程的描述。本文的重点是理解贝尔曼期望方程和贝尔曼最优化方程，并对最优策略求解的数学描述有深刻体会。">
<meta name="keywords" content="强化学习">
<meta property="og:type" content="article">
<meta property="og:title" content="马尔可夫决策过程">
<meta property="og:url" content="http://yoursite.com/2017/09/25/马尔可夫决策过程/index.html">
<meta property="og:site_name" content="野草集">
<meta property="og:description" content="本文主要讲解了马尔科夫决策过程的数学描述以及数学推导，并且为了方便理解马尔科夫决策过程，还添加了对马尔科夫性以及马尔科夫奖励过程的描述。本文的重点是理解贝尔曼期望方程和贝尔曼最优化方程，并对最优策略求解的数学描述有深刻体会。">
<meta property="og:image" content="http://orjn2q9xs.bkt.clouddn.com/2017102502.png">
<meta property="og:image" content="http://orjn2q9xs.bkt.clouddn.com/2017102501.jpg">
<meta property="og:image" content="http://orjn2q9xs.bkt.clouddn.com/2017102503.png">
<meta property="og:image" content="http://orjn2q9xs.bkt.clouddn.com/2017102504.png">
<meta property="og:updated_time" content="2017-09-25T15:43:32.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="马尔可夫决策过程">
<meta name="twitter:description" content="本文主要讲解了马尔科夫决策过程的数学描述以及数学推导，并且为了方便理解马尔科夫决策过程，还添加了对马尔科夫性以及马尔科夫奖励过程的描述。本文的重点是理解贝尔曼期望方程和贝尔曼最优化方程，并对最优策略求解的数学描述有深刻体会。">
<meta name="twitter:image" content="http://orjn2q9xs.bkt.clouddn.com/2017102502.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2017/09/25/马尔可夫决策过程/"/>





  <title>马尔可夫决策过程 | 野草集</title>
  














</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">野草集</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/09/25/马尔可夫决策过程/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="左">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/1.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="野草集">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">马尔可夫决策过程</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-09-25T19:38:45+08:00">
                2017-09-25
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv">本文总阅读量
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>次
            </span>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <blockquote>
<p>本文主要讲解了马尔科夫决策过程的数学描述以及数学推导，并且为了方便理解马尔科夫决策过程，还添加了对马尔科夫性以及马尔科夫奖励过程的描述。本文的重点是理解贝尔曼期望方程和贝尔曼最优化方程，并对最优策略求解的数学描述有深刻体会。</p>
</blockquote>
<a id="more"></a>
<h2 id="马尔可夫模型的几类子模型"><a href="#马尔可夫模型的几类子模型" class="headerlink" title="马尔可夫模型的几类子模型"></a>马尔可夫模型的几类子模型</h2><p>所有的马尔科夫子模型都具备<strong>马尔科夫性</strong>，即系统的下个状态只与当前状态信息有关，而与之前的状态无关。</p>
<script type="math/tex; mode=display">
\mathbb{P}[S_{t+1}|S_t]=\mathbb{P}[S_{t+1}|S_1, \ldots, S_t]</script><p>马尔科夫决策过程（Markov Decision Processes, MDPs）就具有马尔科夫性。所不同的是，马尔科夫决策过程还考虑了动作，即系统的下个状态不仅和当前的状态有关，也和当前采取的动作有关。这里以下棋为例，当我们在某个局面（状态s）走了一步（动作a），这时对手的选择（导致下个状态$s^*$）是不确定的，但是他的选择只与s和a有关，而不用考虑更早之前的状态和动作。</p>
<p>这里再以一个表格来说明各类马尔科夫子模型之间的区别：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>不考虑动作</th>
<th>考虑动作</th>
</tr>
</thead>
<tbody>
<tr>
<td>状态完全可见</td>
<td>马尔科夫链/马尔科夫过程（MC）</td>
<td>马尔科夫决策过程（MDP）</td>
</tr>
<tr>
<td>状态部分可见</td>
<td>隐马尔科夫模型（HMM）</td>
<td>不完全可观察马尔科夫决策过程（POMDP）</td>
</tr>
</tbody>
</table>
</div>
<h2 id="马尔科夫奖励过程"><a href="#马尔科夫奖励过程" class="headerlink" title="马尔科夫奖励过程"></a>马尔科夫奖励过程</h2><p>在正式讨论马尔科夫过程之前，先对马尔科夫奖励过程（Markov Reward Processes, MRPs）作一定分析，以方便后续的讲解。</p>
<p><strong>定义：</strong>一个马尔科夫奖励过程常被描述为一个四元组，形如$(S,P,R, \gamma)$</p>
<ul>
<li>$S$是一个有限的状态集合</li>
<li>$P$是一个状态转移概率矩阵，$\mathbb{P}[S_{t+1}=s^{‘}|S_t=s]$</li>
<li>$R$是一个奖励函数，$R_s=\mathbb{E}[R_{t+1}|S_t=s]$</li>
<li>$\gamma$是折扣因子，$\gamma \in[0,1]$</li>
</ul>
<p>在分析马尔科夫奖励过程的时候，常常需要评定某一个状态的好与坏，这一功能需求通过值函数来实现。值函数会给出某一状态下未来长期的累积奖励之和的期望。特别需要注意对<strong>期望</strong>的把握，这里是对当期状态下所有未来可能的sample作均值计算。值函数的数学公式如下：</p>
<script type="math/tex; mode=display">
v(s)=\mathbb{E}[G_t|S_t=s] \qquad (1)</script><p>其中$G_t$为某一sample的累积奖励之和</p>
<script type="math/tex; mode=display">
G_1 = R_2+\gamma R_3+\ldots+\gamma^{T-2}R_T \qquad(2)</script><p>将上式的$v(s)$展开并变换成如下形式</p>
<script type="math/tex; mode=display">
\begin{align}
    v(s)={} & \mathbb{E}[G_t|S_t=s]\\
          ={} & \mathbb{E}[R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\ldots |S_t=s]\\
          ={} & \mathbb{E}[R_{t+1}+\gamma (R_{t+2} + \gamma R_{t+3} + \ldots)| S_t=s] \\
          ={} & \mathbb{E}[R_{t+1}+\gamma G_{t+1}|S_t=s] \qquad(3)
\end{align}</script><p>因为上式是对$G_t$作均值处理，式子（3）中的$G_{t+1}$可以被$v(S_{t+1})$替代，此时$v(s)$为</p>
<script type="math/tex; mode=display">
v(s)=\mathbb{E}[R_{t+1}+\gamma v(S_{t+1})|S_t = s] \qquad(4)</script><p>该式就被称为马尔科夫奖励过程的Bellman方程，它还有另外一种表达方式</p>
<script type="math/tex; mode=display">
v(s)=R_s + \gamma \sum_{s^{'}\in S} P_{ss^{'}v(s^{'})} \qquad(5)</script><p>其中$P(ss^{‘})$就为状态转移概率。公式（5）的可视化描述可以用下图来表示：</p>
<p><img src="http://orjn2q9xs.bkt.clouddn.com/2017102502.png" alt="Bellman equation for MRPs"></p>
<p>为了方便对Bellman方程进行求解，需要将式（5）转化为矩阵形式，如下所示</p>
<script type="math/tex; mode=display">
v = R + \gamma Pv \qquad (6)</script><p>该式又可表达为</p>
<script type="math/tex; mode=display">
\begin{bmatrix}
v(1)\\
\vdots \\
v(n)
\end{bmatrix}
=
\begin{bmatrix}
R_1\\
\vdots \\
R_n
\end{bmatrix}
+
\gamma
\begin{bmatrix}
    P_{11} & \ldots & P_{1n}\\
    \vdots & \ddots & \vdots \\
        P_{n1} & \ldots & P_{nn}
\end{bmatrix}
\begin{bmatrix}
    v(1)\\
    \vdots\\
    v(n)
\end{bmatrix}</script><p>式（6）的求解式如下</p>
<script type="math/tex; mode=display">
v = (1-\gamma P)^{-1} R \qquad (7)</script><p>在理想情况下，MRPs的求解可以通过式（7）进行解决，但是其计算复杂度太高（$O(n)^3$ for $n$ states），只能应用于小规模的MRPs。所以，在面对大规模的MRPs问题时，可以通过以下三种方法进行求解：</p>
<ul>
<li>Dynamic programming</li>
<li>Monte-Carlo evaluation</li>
<li>Temporal-Difference learning</li>
</ul>
<h2 id="马尔可夫决策过程的数学描述"><a href="#马尔可夫决策过程的数学描述" class="headerlink" title="马尔可夫决策过程的数学描述"></a>马尔可夫决策过程的数学描述</h2><ol>
<li><p>所有的强化学习问题都可以转化为一个马尔科夫决策过程。</p>
</li>
<li><p>一个马尔可夫决策过程（Markov decision process, MDPs）由一个五元组构成，形如$M = (S, A, P, R,\gamma)$</p>
<ul>
<li>S：表示有限状态集，有$s \in S$，$s_i$表示第$i$步的状态</li>
<li>A：表示一组有限动作集合，有$a \in A$，$a_i$表示第$i$步的动作</li>
<li>$P_{ss^{‘}}^a$：表示状态转移概率。$s$表示的是在当前$s\in S$状态下，经过$a\in A$作用后，会转移到的其他状态的概率分布情况，$P_{ss^{‘}}^a=\mathbb{P}[S_{t+1}=s^{‘}|S_t=s, A_t=a]$</li>
<li>R：表示回报函数，$R_s^a = \mathbb{E}[R_{t+1}|S_t=s, A_t=a]$</li>
<li>$\gamma$是折扣因子</li>
</ul>
<p>可以看出MDPs是MRPs的推广，增加了动作的集合。</p>
</li>
<li><p>MDP的动态过程可以这样描述：某个智能体（Agent）的初始状态为$s_0$，然后从A中挑选一个动作$a_0$执行，执行后，智能体按$P_{ss_1}^a$概率随机转移到了下一个$s_1$状态。然后再执行一个动作$a_1$，就转移到了$s_2$，接下来再执行一系列其他动作。我们可以用下面的图来表示状态转移过程：</p>
<p><img src="http://orjn2q9xs.bkt.clouddn.com/2017102501.jpg" alt="MDP"></p>
</li>
<li><p>在马尔可夫决策过程中，还增加了一个新概念——决策（policy），它表示为在给定状态下选取一个动作的概率分布，表达式如下：</p>
<script type="math/tex; mode=display">
\pi(a|s)=\mathbb{P}[A_t=a|S_t=s]</script></li>
<li><p>给定一个马尔科夫决策过程和一个策略$\pi$，其状态序列$S_1,S_2,\ldots$可以看成一个马尔科夫过程$(S, P^{\pi})$，而其状态和奖励序列$S_1, R_2, S_2, \ldots$可以看成一个马尔科夫奖励过程$(S, P^{\pi}, R^{\pi}, \gamma)$，其中$P^{\pi}$和$R^{\pi}$的表达式如下</p>
<script type="math/tex; mode=display">
P^{\pi}_{s,s^{'}}=\sum_{a\in A}\pi(a|s)P^a_{ss^{'}}</script><script type="math/tex; mode=display">
R^{\pi}_s=\sum_{a\in A}\pi(a|s)R^a_s</script><p>​</p>
</li>
</ol>
<h2 id="值函数"><a href="#值函数" class="headerlink" title="值函数"></a>值函数</h2><p>强化学习学到的是一个从环境到动作的映射，记为策略$\pi: S\rightarrow A$，即agent根据当前环境，采取策略$\pi$选取一个动作来执行。然而，强化学习中的当前动作不仅仅影响当前的状态，并且对未来的状态也会产生影响，这就意味着当前动作的立即回报$R^a_s$并不能准确描述这个动作的好与坏，必须将当前动作的长期回报考虑进来，共同决定这一动作的好坏程度。值函数（value function）应运而生，用了刻画当前状态策略$\pi$的长期影响。</p>
<p>值函数分为两种， 一种被称为状态值函数（state-value function），另一种被称为动作值函数（action-value function），其定义式如下：</p>
<script type="math/tex; mode=display">
v_{\pi}(s)=\mathbb{E}_{\pi}[G_t|S_t=s]</script><script type="math/tex; mode=display">
q_{\pi}(s,a)=\mathbb{E}_{\pi}[G_t|S_t=s,A_t=a]</script><p>其中，状态值函数用来描述给定策略$\pi$时状态$s$的好坏程度，动作值函数则用来描述给定策略$\pi$时，在状态$s$下执行动作$a$的好坏程度。</p>
<p>根据马尔科夫奖励过程的Bellman方程，类似地可列出马尔科夫决策过程的Bellman方程，如下所示：</p>
<script type="math/tex; mode=display">
v_{\pi}(s)=\mathbb{E}[R_{t+1}+\gamma v_{\pi}(S_{t+1})|S_t=s]</script><script type="math/tex; mode=display">
q_{\pi}(s,a)=\mathbb{E}[R_{t+1}+\gamma q_{\pi}(S_{t+1},A_{t+1})|S_t=s, A_t=a]</script><p>同理，马尔科夫决策过程的Bellman方程还可以被转化为如下形式</p>
<script type="math/tex; mode=display">
v_{\pi}(s)=\sum_{a\in A}\pi(a|s)q_{\pi}(s,a)</script><script type="math/tex; mode=display">
q_{\pi}(s,a)=R_{s}^a+\gamma \sum_{s^{'}\in S} P_{ss^{'}}^a v_{\pi}(s^{'})</script><p>其可视化过程如下图所示</p>
<p><img src="http://orjn2q9xs.bkt.clouddn.com/2017102503.png" alt="Bellman equation for v"></p>
<p><img src="http://orjn2q9xs.bkt.clouddn.com/2017102504.png" alt="Bellman equation for q"></p>
<p>得到$v_{\pi}(s)$和$q_{\pi}(s,a)$后，互相代入各自方程，得到Bellman方程的另一种形式如下：</p>
<script type="math/tex; mode=display">
v_{\pi}(s)=\sum_{a\in A}\pi(a|s) (R_{s}^a+\gamma \sum_{s^{'}\in S} P_{ss^{'}}^a v_{\pi}(s^{'}))</script><script type="math/tex; mode=display">
q_{\pi}(s,a)=R_{s}^a+\gamma \sum_{s^{'}\in S} P_{ss^{'}}^a (\sum_{a^{'}\in A}\pi(a^{'}|s^{'})q_{\pi}(s^{'},a^{'}))</script><p>然后将上式$v_{\pi}(s)$化成矩阵形式如下：</p>
<script type="math/tex; mode=display">
v_{\pi}=R^{\pi}+\gamma P^{\pi} v_{\pi}</script><p>求解上式得</p>
<script type="math/tex; mode=display">
v_{\pi}=(1-\gamma p^{\pi})^{-1}R^{\pi}</script><h2 id="最优值函数"><a href="#最优值函数" class="headerlink" title="最优值函数"></a>最优值函数</h2><p><strong>最优状态值函数</strong>的定义式如下：</p>
<script type="math/tex; mode=display">
v_*(s)=\max_{\pi}v_{\pi}(s)</script><p><strong>最优动作值函数</strong>的定义式如下：</p>
<script type="math/tex; mode=display">
q_*(s,a)=\max_{\pi}q_{\pi}(s,a)</script><p>另外，以上两个值函数还可以通过下式联系在一起，形成<strong>Bellman最优化方程</strong></p>
<script type="math/tex; mode=display">
v_*(s)=\max_{a}q_*(s,a)=\max_{a}R_{s}^a+\gamma \sum_{s^{'}\in S} P_{ss^{'}}^a v_*(s^{'})</script><script type="math/tex; mode=display">
q_*(s,a)=R_{s}^a+\gamma \sum_{s^{'}\in S} P_{ss^{'}}^a v_*(s^{'})=R_{s}^a+\gamma \sum_{s^{'}\in S} P_{ss^{'}}^a\max_{a^{'}}q_*(s^{'},a^{'})</script><p>在这里还需要注意，Bellman最优化方程没有显式解，并且是非线性的，通常有以下几种方法来解决这一问题：</p>
<ul>
<li>Value Iteration</li>
<li>Policy Iteration</li>
<li>Q-learning</li>
<li>Sarsa</li>
</ul>
<h2 id="最优策略的求解"><a href="#最优策略的求解" class="headerlink" title="最优策略的求解"></a>最优策略的求解</h2><p>最优策略可通过最优动作值函数$q_*(a|s)$来寻找，其定义式如下：</p>
<script type="math/tex; mode=display">
\pi_*(a|s)=
\begin{cases}
    1 & \text{if } a=arg\max_{a \in A} q_*(s,a)\\
    0 & \text{otherwise}
\end{cases}</script><h2 id="马尔科夫决策过程的推广"><a href="#马尔科夫决策过程的推广" class="headerlink" title="马尔科夫决策过程的推广"></a>马尔科夫决策过程的推广</h2><p>在现实生活中，我们可能不会遇到标准的马尔科夫决策过程，这些过程通常有以下几种：</p>
<ul>
<li>无限和连续的MDPs</li>
<li>部分观察的MDPs（POMDPs）</li>
<li>未折现或采取平均奖励的MDPs   </li>
</ul>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/强化学习/" rel="tag"># 强化学习</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/09/10/Ubuntu下CUDA和cuDNN的安装/" rel="next" title="Ubuntu下CUDA和cuDNN的安装">
                <i class="fa fa-chevron-left"></i> Ubuntu下CUDA和cuDNN的安装
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/10/15/TexLive的安装/" rel="prev" title="TexLive的安装">
                TexLive的安装 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/1.jpg"
               alt="左" />
          <p class="site-author-name" itemprop="name">左</p>
           
              <p class="site-description motion-element" itemprop="description">那就热爱生活吧</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives/">
                <span class="site-state-item-count">6</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">2</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#马尔可夫模型的几类子模型"><span class="nav-number">1.</span> <span class="nav-text">马尔可夫模型的几类子模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#马尔科夫奖励过程"><span class="nav-number">2.</span> <span class="nav-text">马尔科夫奖励过程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#马尔可夫决策过程的数学描述"><span class="nav-number">3.</span> <span class="nav-text">马尔可夫决策过程的数学描述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#值函数"><span class="nav-number">4.</span> <span class="nav-text">值函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#最优值函数"><span class="nav-number">5.</span> <span class="nav-text">最优值函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#最优策略的求解"><span class="nav-number">6.</span> <span class="nav-text">最优策略的求解</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#马尔科夫决策过程的推广"><span class="nav-number">7.</span> <span class="nav-text">马尔科夫决策过程的推广</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">左</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Gemini
  </a>
</div>


        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      本站访客数
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      人次
    </span>
  

  
    <span class="site-pv">
      本站总访问量
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      次
    </span>
  
</div>


        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.2"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script>



  


  




	





  





  






  





  

  

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
