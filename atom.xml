<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>野草集</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2017-10-17T11:29:49.000Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>左</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>强化学习之蒙特卡洛方法</title>
    <link href="http://yoursite.com/2017/10/17/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B9%8B%E8%92%99%E7%89%B9%E5%8D%A1%E6%B4%9B%E6%96%B9%E6%B3%95/"/>
    <id>http://yoursite.com/2017/10/17/强化学习之蒙特卡洛方法/</id>
    <published>2017-10-17T11:07:34.000Z</published>
    <updated>2017-10-17T11:29:49.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p> 本文主要介绍强化学习中涉及的蒙特卡洛方法，该方法是强化学习求解免模型（model-free）方法中的重要组成部分，主要分为同策略（on-policy）和异策略（off-policy）两个分支。另外，在具体求解Q函数的时候，还分为first-visit和all-visit两种计算技巧。</p></blockquote><a id="more"></a><h2 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h2><ol><li><strong>什么是蒙特卡洛方法？</strong>简单来说，蒙特卡洛方法就是利用统计学的思想来对Q函数进行求解，采用<strong>平均累积奖赏作为期望累积奖赏的近似</strong>。这样一来，就避免了大量的数值计算过程，并且还能够解决环境模型未知的情况。</li><li>蒙特卡洛方法同样具有其局限性，它只能够解决有限状态下的强化学习任务，即每一次episode都有一个终止状态。</li><li>值得注意的是，蒙特卡洛方法<strong>只涉及到Q函数的求解</strong>，并不涉及V函数的求解。这是因为通过Q函数我们可以很快得到最优策略$\pi$，而又因为此时环境未知，无法从V函数快速得到Q函数，所以此时对V函数求解意义不大。</li></ol><h2 id="方法描述"><a href="#方法描述" class="headerlink" title="方法描述"></a>方法描述</h2><p>在模型未知的情况下，从初始状态出发，使用某种策略（通常初始策略是$\pi(a|s)=\frac{1}{A(s)}$）进行采样，执行该策略获得轨迹</p><script type="math/tex; mode=display"><x_0, a_0, r_1, x_1, a_1, r_2, \ldots, x_{T-1}, a_{T-1}, r_T, x_T></script><p>然后，对轨迹中出现的每一对状态-动作，记录其后的奖赏之和，作为该状态-动作对的一次累积奖赏采样值。多次采样得到多条轨迹后，将每个状态-动作对的累积奖赏值进行平均，即得到状态-动作值函数（Q函数）的估计。</p><h2 id="同策略方法"><a href="#同策略方法" class="headerlink" title="同策略方法"></a>同策略方法</h2><p>可以看出，要想得到Q函数的良好近似，就需要多次采样以得到不同的轨迹，以期遍历所有的<strong>状态空间和动作空间</strong>。同时，在实际操作中，可能我们每次迭代得到的策略是确定性的，即$\pi(a|s)=1$，或$\pi=arg \max_aQ(s,a)$，这样带来的后果就是我们无法遍历所有的状态空间和动作空间，面临“仅利用”的困境。为了跳出这一困境，就需要制定一种新的策略，使得所有动作都有可能被选中，这种新策略的数学定义如下：</p><script type="math/tex; mode=display">\pi^{\varepsilon}(x)=\begin{cases}    \pi(s) &\text{以概率}1-\varepsilon \\    \text{A中以均匀概率选取的动作} &\text{以概率}\varepsilon\end{cases}</script><p>其中，$\pi(s)$被称为<strong>原始策略</strong>，即算法每次迭代后得出的策略，也是最后所要求的策略，$\pi^{\varepsilon}(s)$被称为<strong>$\varepsilon$贪心策略</strong>。从新策略的数学定义可以看出，当前最优动作被选中的概率是$1-\varepsilon+\frac{\varepsilon}{|A|}$，而每个非最优的动作被选中的概率是$\frac{\varepsilon}{|A|}$。 </p><p><img src="http://orjn2q9xs.bkt.clouddn.com/On-Policy%20MC.png" alt="通策略蒙特卡洛算法"></p><p>同策略蒙特卡洛学习算法生成的是$\varepsilon$贪心策略，而我们要求的是原始策略，能否在策略评估的时候使用$\varepsilon$贪心策略，而在策略改进的使用使用原始策略呢？答案是肯定的，这就导致了异策略算法的发明。</p><h2 id="异策略方法"><a href="#异策略方法" class="headerlink" title="异策略方法"></a>异策略方法</h2><h3 id="重要性采样"><a href="#重要性采样" class="headerlink" title="重要性采样"></a>重要性采样</h3><p>函数$f(x)$在概率分布$p$下的期望可表达为</p><script type="math/tex; mode=display">\mathbb{E}[f]=\int_x p(x)f(x)dx</script><p>可通过从概率分布$p$上的采样${x_1,x_2,\ldots,x_m}$来估计$f$的期望，即</p><script type="math/tex; mode=display">\hat{\mathbb{E}}[f]=\frac{1}{m} \sum_{i=1}^{m} f(x_i)</script><p>这时候引入另一个分布$q$，则函数$f$在概率分布$p$下的期望也可等价地写为</p><script type="math/tex; mode=display">\mathbb{E}[f]=\int_x q(x)\frac{p(x)}{q(x)}f(x)dx</script><p>通过在$q$上的采样${x_1^{‘}, x_2^{‘}, \ldots, x_m^{‘}}$可估计$f$的期望如下：</p><script type="math/tex; mode=display">\hat{\mathbb{E}}[f]=\frac{1}{m} \sum_{i=1}^{m} \frac{p(x_i^{'})}{q(x_i^{'})} f(x_i^{'})</script><p>这样的基于一个分布的采样来估计另一个分布下的期望称为<strong>重要性采样</strong>。</p><h3 id="算法推导"><a href="#算法推导" class="headerlink" title="算法推导"></a>算法推导</h3><p>使用策略$\pi$的采样轨迹来评估策略$\pi$，实际上就是用累积奖赏估计期望</p><script type="math/tex; mode=display">Q(x,a)=\frac{1}{m} \sum_{i=1}^{m}R_i</script><p>由上述的重要性采样定理可知，我们可以通过策略$\pi^{‘}$的采样轨迹来评估策略$\pi$，则仅需对累积奖赏加权</p><script type="math/tex; mode=display">Q(x,a)=\frac{1}{m} \sum_{i=1}^{m} \frac{P^{\pi}_i}{P^{\pi^{'}}_i} R_i</script><p>其中，$P^{\pi}_i$和$P^{\pi^{‘}}_i$分别表示两个策略产生第$i$条轨迹的概率。对于一条给定的轨迹，策略$\pi$产生该轨迹的概率为</p><script type="math/tex; mode=display">P^{\pi}=\prod_{i=0}^{T-1} \pi(x_i,a_i)P^{a_i}_{x_i \rightarrow x_{i+1}}</script><p>由此可以计算得出$P^{\pi}_i$和$P^{\pi^{‘}}_i$的比值为</p><script type="math/tex; mode=display">\frac{P^{\pi}}{P^{\pi^{'}}}=\prod_{i=0}^{T-1}\frac{\pi(x_i,a_i)}{\pi^{'}(x_i,a_i)}</script><p>异策略蒙特卡洛算法就是在策略评估的时候使用的是$\varepsilon$贪心策略，而在策略改进的时候使用的是原始策略，这样最后算法得到的就是原始策略，符合我们的要求。其算法流程如下所示：</p><p><img src="http://orjn2q9xs.bkt.clouddn.com/Off-Policy%20MC.png" alt="异策略蒙特卡洛学习"></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt; 本文主要介绍强化学习中涉及的蒙特卡洛方法，该方法是强化学习求解免模型（model-free）方法中的重要组成部分，主要分为同策略（on-policy）和异策略（off-policy）两个分支。另外，在具体求解Q函数的时候，还分为first-visit和all-visit两种计算技巧。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="强化学习" scheme="http://yoursite.com/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>强化学习之策略迭代方法和值迭代方法</title>
    <link href="http://yoursite.com/2017/10/16/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B9%8B%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3%E6%96%B9%E6%B3%95%E5%92%8C%E5%80%BC%E8%BF%AD%E4%BB%A3%E6%96%B9%E6%B3%95/"/>
    <id>http://yoursite.com/2017/10/16/强化学习之策略迭代方法和值迭代方法/</id>
    <published>2017-10-16T09:18:10.000Z</published>
    <updated>2017-10-16T10:44:46.000Z</updated>
    
    <content type="html"><![CDATA[<p>在马尔科夫决策过程$(S, A, P, R,\gamma )$所有参数全部知道的情况下，要想求解Bellman最优化方程，需要运用到动态规划（Dynamic Programming）的思想，有两种方法，一种被叫作基于策略的迭代，另一种是基于值函数的迭代。下面分别阐述之。<br><a id="more"></a></p><h2 id="策略迭代（Policy-Iteration）"><a href="#策略迭代（Policy-Iteration）" class="headerlink" title="策略迭代（Policy Iteration）"></a>策略迭代（Policy Iteration）</h2><p>基于策略的迭代基本上可以看做是对<strong>贝尔曼期望方程</strong>不断迭代的一个求解过程，贝尔曼期望方程如下所示：</p><script type="math/tex; mode=display">v_{\pi}(s)=\sum_{a\in A} \pi(a|s)[R_s^a+\gamma\sum_{s^{'}\in S}P_{ss^{'}}^a v_{\pi}(s^{'})]</script><p>其伪代码如下所示：</p><ol><li><p>Initialization</p><p>$v_{\pi}(s) \in \mathbb{R}$ and $\pi(a|s)\in [0,1], a\in A$ arbitrarily for all $s\in S$</p></li><li><p>Policy Evaluation</p><p>Repeat</p><p>$\varDelta \leftarrow 0$</p><p>   For each $s\in S$</p><p>   $v \leftarrow v_{\pi}(s)$</p><p>   $v_{\pi}(s) \leftarrow \sum_{a\in A} \pi(a|s)[R_s^a+\gamma\sum_{s^{‘}\in S}P_{ss^{‘}}^a v_{\pi}(s^{‘})]$</p><p>   $\varDelta \leftarrow max(\varDelta, |v-v_{\pi}(s)|)$</p><p>Until $\varDelta &lt; \theta$ ($\theta$ is a small positive number)</p></li><li><p>Policy Improvement</p><p>policy-stable $\leftarrow$ true</p><p>For each $s\in S$</p><p>​    $a\leftarrow \pi(s)$</p><p>​    $\pi(s)\leftarrow arg \max_{a\in A} R_s^a+\gamma\sum_{s^{‘}\in S}P_{ss^{‘}}^a v_{\pi}(s^{‘})$</p><p>​    If $a \neq \pi(s)$, then policy-stable$\leftarrow$false</p><p>If policy-stable, then stop and return $v_{\pi}$ and $\pi$; else go to 2</p></li></ol><p>从上述伪代码可以看出，基于策略的迭代分为两个步骤，一个是策略评估，一个是策略改进，并且最后该程序一定会收敛，并且收敛的时候，$v_{\pi} = v_{*}, \pi = \pi_{*}$。</p><h2 id="值迭代（Value-Iteration）"><a href="#值迭代（Value-Iteration）" class="headerlink" title="值迭代（Value Iteration）"></a>值迭代（Value Iteration）</h2><p>值迭代则是使用<strong>贝尔曼最优化方程</strong>而不是贝尔曼期望方程得到的，贝尔曼最优化方程如下所示：</p><script type="math/tex; mode=display">v_{*}(s)=\max_{a\in A}R_s^a+\gamma \sum_{s^{'}\in S}P_{ss^{'}}^a v_{*}(s^{'})</script><p>其伪代码如下所示：</p><p>Initialize array $v_{*}(s)$ arbitrarily (e.g., v(s) = 0 for all $s\in S$)</p><p>Repeat</p><p>​    $\varDelta \leftarrow 0$</p><p>​    For each $s\in S$</p><p>​        $v \leftarrow v_{*}(s)$    </p><p>​        $v_{*}(s) \leftarrow \max_{a\in A} R_s^a+\gamma\sum_{s^{‘}\in S}P_{ss^{‘}}^a v_{*}(s^{‘})$     </p><p>​        $\varDelta \leftarrow max(\varDelta,|v-v_{*}(s)|)$</p><p>​    until $\varDelta &lt; \theta$ ($\theta$ is a small positive number)</p><p>Output a deterministic policy, $\pi$, such that</p><script type="math/tex; mode=display">\pi(s)= arg \max_{a\in A} R_s^a+\gamma\sum_{s^{'}\in S}P_{ss^{'}}^a v_{*}(s^{'})</script><p>可以看出，基于值函数的迭代并不需要提前知道策略是什么，其算法流程相比于基于策略的迭代更为简洁和紧凑。</p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>基于策略的迭代和基于值函数的迭代各有优势，并且适应于各自的领域。基于策略的迭代输出的是采取每种动作的概率值，而基于值函数的迭代输出的是一个确定的动作，所以，基于策略的迭代适合于连续动作空间，而基于值函数的迭代适合于离散动作空间。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在马尔科夫决策过程$(S, A, P, R,\gamma )$所有参数全部知道的情况下，要想求解Bellman最优化方程，需要运用到动态规划（Dynamic Programming）的思想，有两种方法，一种被叫作基于策略的迭代，另一种是基于值函数的迭代。下面分别阐述之。&lt;br&gt;
    
    </summary>
    
    
      <category term="强化学习" scheme="http://yoursite.com/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>TexLive的安装</title>
    <link href="http://yoursite.com/2017/10/15/TexLive%E7%9A%84%E5%AE%89%E8%A3%85/"/>
    <id>http://yoursite.com/2017/10/15/TexLive的安装/</id>
    <published>2017-10-15T15:24:58.000Z</published>
    <updated>2017-10-15T15:33:43.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="TexLive的安装"><a href="#TexLive的安装" class="headerlink" title="TexLive的安装"></a>TexLive的安装</h2><blockquote><p>安装环境：Ubuntu 16.04 LTS</p></blockquote><h3 id="在线安装"><a href="#在线安装" class="headerlink" title="在线安装"></a>在线安装</h3><p>在命令行工具界面输入命令：<code>sudo apt-get install texlive-full</code>即可。需要下载的文件大概有好几个G，安装时间会比较长。</p><a id="more"></a><h3 id="离线安装"><a href="#离线安装" class="headerlink" title="离线安装"></a>离线安装</h3><p><strong>第一步</strong>：下载镜像，推荐下载地址为<a href="https://mirrors.tuna.tsinghua.edu.cn/" target="_blank" rel="external">清华大学开源软件镜像站</a>。</p><ul><li>进入站点</li></ul><p><img src="http://orjn2q9xs.bkt.clouddn.com/2017101501.png" alt="进入开源镜像站点"></p><ul><li>获取安装镜像</li></ul><p><img src="http://orjn2q9xs.bkt.clouddn.com/2017101502.png" alt="获取下载链接"></p><p><strong>第二步</strong>：安装</p><ul><li><p>安装<code>perl-tk</code>以便启动可视化安装界面</p><p><code>sudo apt-get install perl-tk</code></p></li><li><p>进入镜像文件所在文件夹，挂载镜像文件：</p><p><code>sudo mount -t iso9660 -o loop texlive*.iso /mnt/</code></p></li><li><p>执行安装：</p><p><code>cd /mnt/</code></p><p><code>sudo ./install-tl --gui=perltk</code></p></li><li><p>安装过程：</p><p>将create symlinks in system directories后面的选项设置为Yes，如下图所示，操作顺序：青－&gt; 红－&gt; 蓝－&gt; 黄。等待安装结束即可。(安装所需的磁盘空间较大，在安装之前确保本地有足够的内存)</p><p><img src="http://orjn2q9xs.bkt.clouddn.com/2017101503.png" alt="texlive-install"></p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;TexLive的安装&quot;&gt;&lt;a href=&quot;#TexLive的安装&quot; class=&quot;headerlink&quot; title=&quot;TexLive的安装&quot;&gt;&lt;/a&gt;TexLive的安装&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;安装环境：Ubuntu 16.04 LTS&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;在线安装&quot;&gt;&lt;a href=&quot;#在线安装&quot; class=&quot;headerlink&quot; title=&quot;在线安装&quot;&gt;&lt;/a&gt;在线安装&lt;/h3&gt;&lt;p&gt;在命令行工具界面输入命令：&lt;code&gt;sudo apt-get install texlive-full&lt;/code&gt;即可。需要下载的文件大概有好几个G，安装时间会比较长。&lt;/p&gt;
    
    </summary>
    
    
      <category term="软件安装" scheme="http://yoursite.com/tags/%E8%BD%AF%E4%BB%B6%E5%AE%89%E8%A3%85/"/>
    
  </entry>
  
  <entry>
    <title>马尔可夫决策过程</title>
    <link href="http://yoursite.com/2017/09/25/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/"/>
    <id>http://yoursite.com/2017/09/25/马尔可夫决策过程/</id>
    <published>2017-09-25T11:38:45.000Z</published>
    <updated>2017-09-25T15:43:32.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>本文主要讲解了马尔科夫决策过程的数学描述以及数学推导，并且为了方便理解马尔科夫决策过程，还添加了对马尔科夫性以及马尔科夫奖励过程的描述。本文的重点是理解贝尔曼期望方程和贝尔曼最优化方程，并对最优策略求解的数学描述有深刻体会。</p></blockquote><a id="more"></a><h2 id="马尔可夫模型的几类子模型"><a href="#马尔可夫模型的几类子模型" class="headerlink" title="马尔可夫模型的几类子模型"></a>马尔可夫模型的几类子模型</h2><p>所有的马尔科夫子模型都具备<strong>马尔科夫性</strong>，即系统的下个状态只与当前状态信息有关，而与之前的状态无关。</p><script type="math/tex; mode=display">\mathbb{P}[S_{t+1}|S_t]=\mathbb{P}[S_{t+1}|S_1, \ldots, S_t]</script><p>马尔科夫决策过程（Markov Decision Processes, MDPs）就具有马尔科夫性。所不同的是，马尔科夫决策过程还考虑了动作，即系统的下个状态不仅和当前的状态有关，也和当前采取的动作有关。这里以下棋为例，当我们在某个局面（状态s）走了一步（动作a），这时对手的选择（导致下个状态$s^*$）是不确定的，但是他的选择只与s和a有关，而不用考虑更早之前的状态和动作。</p><p>这里再以一个表格来说明各类马尔科夫子模型之间的区别：</p><div class="table-container"><table><thead><tr><th></th><th>不考虑动作</th><th>考虑动作</th></tr></thead><tbody><tr><td>状态完全可见</td><td>马尔科夫链/马尔科夫过程（MC）</td><td>马尔科夫决策过程（MDP）</td></tr><tr><td>状态部分可见</td><td>隐马尔科夫模型（HMM）</td><td>不完全可观察马尔科夫决策过程（POMDP）</td></tr></tbody></table></div><h2 id="马尔科夫奖励过程"><a href="#马尔科夫奖励过程" class="headerlink" title="马尔科夫奖励过程"></a>马尔科夫奖励过程</h2><p>在正式讨论马尔科夫过程之前，先对马尔科夫奖励过程（Markov Reward Processes, MRPs）作一定分析，以方便后续的讲解。</p><p><strong>定义：</strong>一个马尔科夫奖励过程常被描述为一个四元组，形如$(S,P,R, \gamma)$</p><ul><li>$S$是一个有限的状态集合</li><li>$P$是一个状态转移概率矩阵，$\mathbb{P}[S_{t+1}=s^{‘}|S_t=s]$</li><li>$R$是一个奖励函数，$R_s=\mathbb{E}[R_{t+1}|S_t=s]$</li><li>$\gamma$是折扣因子，$\gamma \in[0,1]$</li></ul><p>在分析马尔科夫奖励过程的时候，常常需要评定某一个状态的好与坏，这一功能需求通过值函数来实现。值函数会给出某一状态下未来长期的累积奖励之和的期望。特别需要注意对<strong>期望</strong>的把握，这里是对当期状态下所有未来可能的sample作均值计算。值函数的数学公式如下：</p><script type="math/tex; mode=display">v(s)=\mathbb{E}[G_t|S_t=s] \qquad (1)</script><p>其中$G_t$为某一sample的累积奖励之和</p><script type="math/tex; mode=display">G_1 = R_2+\gamma R_3+\ldots+\gamma^{T-2}R_T \qquad(2)</script><p>将上式的$v(s)$展开并变换成如下形式</p><script type="math/tex; mode=display">\begin{align}    v(s)={} & \mathbb{E}[G_t|S_t=s]\\          ={} & \mathbb{E}[R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\ldots |S_t=s]\\          ={} & \mathbb{E}[R_{t+1}+\gamma (R_{t+2} + \gamma R_{t+3} + \ldots)| S_t=s] \\          ={} & \mathbb{E}[R_{t+1}+\gamma G_{t+1}|S_t=s] \qquad(3)\end{align}</script><p>因为上式是对$G_t$作均值处理，式子（3）中的$G_{t+1}$可以被$v(S_{t+1})$替代，此时$v(s)$为</p><script type="math/tex; mode=display">v(s)=\mathbb{E}[R_{t+1}+\gamma v(S_{t+1})|S_t = s] \qquad(4)</script><p>该式就被称为马尔科夫奖励过程的Bellman方程，它还有另外一种表达方式</p><script type="math/tex; mode=display">v(s)=R_s + \gamma \sum_{s^{'}\in S} P_{ss^{'}v(s^{'})} \qquad(5)</script><p>其中$P(ss^{‘})$就为状态转移概率。公式（5）的可视化描述可以用下图来表示：</p><p><img src="http://orjn2q9xs.bkt.clouddn.com/2017102502.png" alt="Bellman equation for MRPs"></p><p>为了方便对Bellman方程进行求解，需要将式（5）转化为矩阵形式，如下所示</p><script type="math/tex; mode=display">v = R + \gamma Pv \qquad (6)</script><p>该式又可表达为</p><script type="math/tex; mode=display">\begin{bmatrix}v(1)\\\vdots \\v(n)\end{bmatrix}=\begin{bmatrix}R_1\\\vdots \\R_n\end{bmatrix}+\gamma\begin{bmatrix}    P_{11} & \ldots & P_{1n}\\    \vdots & \ddots & \vdots \\        P_{n1} & \ldots & P_{nn}\end{bmatrix}\begin{bmatrix}    v(1)\\    \vdots\\    v(n)\end{bmatrix}</script><p>式（6）的求解式如下</p><script type="math/tex; mode=display">v = (1-\gamma P)^{-1} R \qquad (7)</script><p>在理想情况下，MRPs的求解可以通过式（7）进行解决，但是其计算复杂度太高（$O(n)^3$ for $n$ states），只能应用于小规模的MRPs。所以，在面对大规模的MRPs问题时，可以通过以下三种方法进行求解：</p><ul><li>Dynamic programming</li><li>Monte-Carlo evaluation</li><li>Temporal-Difference learning</li></ul><h2 id="马尔可夫决策过程的数学描述"><a href="#马尔可夫决策过程的数学描述" class="headerlink" title="马尔可夫决策过程的数学描述"></a>马尔可夫决策过程的数学描述</h2><ol><li><p>所有的强化学习问题都可以转化为一个马尔科夫决策过程。</p></li><li><p>一个马尔可夫决策过程（Markov decision process, MDPs）由一个五元组构成，形如$M = (S, A, P, R,\gamma)$</p><ul><li>S：表示有限状态集，有$s \in S$，$s_i$表示第$i$步的状态</li><li>A：表示一组有限动作集合，有$a \in A$，$a_i$表示第$i$步的动作</li><li>$P_{ss^{‘}}^a$：表示状态转移概率。$s$表示的是在当前$s\in S$状态下，经过$a\in A$作用后，会转移到的其他状态的概率分布情况，$P_{ss^{‘}}^a=\mathbb{P}[S_{t+1}=s^{‘}|S_t=s, A_t=a]$</li><li>R：表示回报函数，$R_s^a = \mathbb{E}[R_{t+1}|S_t=s, A_t=a]$</li><li>$\gamma$是折扣因子</li></ul><p>可以看出MDPs是MRPs的推广，增加了动作的集合。</p></li><li><p>MDP的动态过程可以这样描述：某个智能体（Agent）的初始状态为$s_0$，然后从A中挑选一个动作$a_0$执行，执行后，智能体按$P_{ss_1}^a$概率随机转移到了下一个$s_1$状态。然后再执行一个动作$a_1$，就转移到了$s_2$，接下来再执行一系列其他动作。我们可以用下面的图来表示状态转移过程：</p><p><img src="http://orjn2q9xs.bkt.clouddn.com/2017102501.jpg" alt="MDP"></p></li><li><p>在马尔可夫决策过程中，还增加了一个新概念——决策（policy），它表示为在给定状态下选取一个动作的概率分布，表达式如下：</p><script type="math/tex; mode=display">\pi(a|s)=\mathbb{P}[A_t=a|S_t=s]</script></li><li><p>给定一个马尔科夫决策过程和一个策略$\pi$，其状态序列$S_1,S_2,\ldots$可以看成一个马尔科夫过程$(S, P^{\pi})$，而其状态和奖励序列$S_1, R_2, S_2, \ldots$可以看成一个马尔科夫奖励过程$(S, P^{\pi}, R^{\pi}, \gamma)$，其中$P^{\pi}$和$R^{\pi}$的表达式如下</p><script type="math/tex; mode=display">P^{\pi}_{s,s^{'}}=\sum_{a\in A}\pi(a|s)P^a_{ss^{'}}</script><script type="math/tex; mode=display">R^{\pi}_s=\sum_{a\in A}\pi(a|s)R^a_s</script><p>​</p></li></ol><h2 id="值函数"><a href="#值函数" class="headerlink" title="值函数"></a>值函数</h2><p>强化学习学到的是一个从环境到动作的映射，记为策略$\pi: S\rightarrow A$，即agent根据当前环境，采取策略$\pi$选取一个动作来执行。然而，强化学习中的当前动作不仅仅影响当前的状态，并且对未来的状态也会产生影响，这就意味着当前动作的立即回报$R^a_s$并不能准确描述这个动作的好与坏，必须将当前动作的长期回报考虑进来，共同决定这一动作的好坏程度。值函数（value function）应运而生，用了刻画当前状态策略$\pi$的长期影响。</p><p>值函数分为两种， 一种被称为状态值函数（state-value function），另一种被称为动作值函数（action-value function），其定义式如下：</p><script type="math/tex; mode=display">v_{\pi}(s)=\mathbb{E}_{\pi}[G_t|S_t=s]</script><script type="math/tex; mode=display">q_{\pi}(s,a)=\mathbb{E}_{\pi}[G_t|S_t=s,A_t=a]</script><p>其中，状态值函数用来描述给定策略$\pi$时状态$s$的好坏程度，动作值函数则用来描述给定策略$\pi$时，在状态$s$下执行动作$a$的好坏程度。</p><p>根据马尔科夫奖励过程的Bellman方程，类似地可列出马尔科夫决策过程的Bellman方程，如下所示：</p><script type="math/tex; mode=display">v_{\pi}(s)=\mathbb{E}[R_{t+1}+\gamma v_{\pi}(S_{t+1})|S_t=s]</script><script type="math/tex; mode=display">q_{\pi}(s,a)=\mathbb{E}[R_{t+1}+\gamma q_{\pi}(S_{t+1},A_{t+1})|S_t=s, A_t=a]</script><p>同理，马尔科夫决策过程的Bellman方程还可以被转化为如下形式</p><script type="math/tex; mode=display">v_{\pi}(s)=\sum_{a\in A}\pi(a|s)q_{\pi}(s,a)</script><script type="math/tex; mode=display">q_{\pi}(s,a)=R_{s}^a+\gamma \sum_{s^{'}\in S} P_{ss^{'}}^a v_{\pi}(s^{'})</script><p>其可视化过程如下图所示</p><p><img src="http://orjn2q9xs.bkt.clouddn.com/2017102503.png" alt="Bellman equation for v"></p><p><img src="http://orjn2q9xs.bkt.clouddn.com/2017102504.png" alt="Bellman equation for q"></p><p>得到$v_{\pi}(s)$和$q_{\pi}(s,a)$后，互相代入各自方程，得到Bellman方程的另一种形式如下：</p><script type="math/tex; mode=display">v_{\pi}(s)=\sum_{a\in A}\pi(a|s) (R_{s}^a+\gamma \sum_{s^{'}\in S} P_{ss^{'}}^a v_{\pi}(s^{'}))</script><script type="math/tex; mode=display">q_{\pi}(s,a)=R_{s}^a+\gamma \sum_{s^{'}\in S} P_{ss^{'}}^a (\sum_{a^{'}\in A}\pi(a^{'}|s^{'})q_{\pi}(s^{'},a^{'}))</script><p>然后将上式$v_{\pi}(s)$化成矩阵形式如下：</p><script type="math/tex; mode=display">v_{\pi}=R^{\pi}+\gamma P^{\pi} v_{\pi}</script><p>求解上式得</p><script type="math/tex; mode=display">v_{\pi}=(1-\gamma p^{\pi})^{-1}R^{\pi}</script><h2 id="最优值函数"><a href="#最优值函数" class="headerlink" title="最优值函数"></a>最优值函数</h2><p><strong>最优状态值函数</strong>的定义式如下：</p><script type="math/tex; mode=display">v_*(s)=\max_{\pi}v_{\pi}(s)</script><p><strong>最优动作值函数</strong>的定义式如下：</p><script type="math/tex; mode=display">q_*(s,a)=\max_{\pi}q_{\pi}(s,a)</script><p>另外，以上两个值函数还可以通过下式联系在一起，形成<strong>Bellman最优化方程</strong></p><script type="math/tex; mode=display">v_*(s)=\max_{a}q_*(s,a)=\max_{a}R_{s}^a+\gamma \sum_{s^{'}\in S} P_{ss^{'}}^a v_*(s^{'})</script><script type="math/tex; mode=display">q_*(s,a)=R_{s}^a+\gamma \sum_{s^{'}\in S} P_{ss^{'}}^a v_*(s^{'})=R_{s}^a+\gamma \sum_{s^{'}\in S} P_{ss^{'}}^a\max_{a^{'}}q_*(s^{'},a^{'})</script><p>在这里还需要注意，Bellman最优化方程没有显式解，并且是非线性的，通常有以下几种方法来解决这一问题：</p><ul><li>Value Iteration</li><li>Policy Iteration</li><li>Q-learning</li><li>Sarsa</li></ul><h2 id="最优策略的求解"><a href="#最优策略的求解" class="headerlink" title="最优策略的求解"></a>最优策略的求解</h2><p>最优策略可通过最优动作值函数$q_*(a|s)$来寻找，其定义式如下：</p><script type="math/tex; mode=display">\pi_*(a|s)=\begin{cases}    1 & \text{if } a=arg\max_{a \in A} q_*(s,a)\\    0 & \text{otherwise}\end{cases}</script><h2 id="马尔科夫决策过程的推广"><a href="#马尔科夫决策过程的推广" class="headerlink" title="马尔科夫决策过程的推广"></a>马尔科夫决策过程的推广</h2><p>在现实生活中，我们可能不会遇到标准的马尔科夫决策过程，这些过程通常有以下几种：</p><ul><li>无限和连续的MDPs</li><li>部分观察的MDPs（POMDPs）</li><li>未折现或采取平均奖励的MDPs   </li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文主要讲解了马尔科夫决策过程的数学描述以及数学推导，并且为了方便理解马尔科夫决策过程，还添加了对马尔科夫性以及马尔科夫奖励过程的描述。本文的重点是理解贝尔曼期望方程和贝尔曼最优化方程，并对最优策略求解的数学描述有深刻体会。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="强化学习" scheme="http://yoursite.com/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>Ubuntu下CUDA和cuDNN的安装</title>
    <link href="http://yoursite.com/2017/09/10/Ubuntu%E4%B8%8BCUDA%E5%92%8CcuDNN%E7%9A%84%E5%AE%89%E8%A3%85/"/>
    <id>http://yoursite.com/2017/09/10/Ubuntu下CUDA和cuDNN的安装/</id>
    <published>2017-09-10T05:58:33.000Z</published>
    <updated>2017-09-10T06:06:36.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>前提条件：已安装NVIDIA显卡驱动<br>安装环境：Ubuntu 16.04 LTS<br>Cuda版本：cuda 8.0<br>cuDNN版本：cuDNN 5.1</p></blockquote><h2 id="安装Cuda"><a href="#安装Cuda" class="headerlink" title="安装Cuda"></a>安装Cuda</h2><ol><li>下载cuda的run安装文件；<a id="more"></a></li><li><p>进入安装文件所在的文件夹，执行<code>sudo ./cuda_xxxxx.run</code>命令；</p></li><li><p>运行安装命令后会有很多选项需要我们进行选择，第一个选项输入<code>accept</code><br>,第二个选项输入<code>no</code>（因为已经安装了nvidia显卡驱动）,其他选项输入<code>yes</code>即可；</p></li><li><p>打开<code>.bashrc</code>文件，在文件末尾追加一下几行，配置cuda环境变量：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">export LD_LIBRARY_PATH=&quot;$LD_LIBRARY_PATH:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64&quot;</div><div class="line">export CUDA_HOME=/usr/local/cuda</div><div class="line">export PATH=/usr/local/cuda-8.0/bin:$PATH</div></pre></td></tr></table></figure></li><li><p>关闭<code>.bashrc</code>文件，然后在命令行界面输入<code>source .bashrc</code>更新设置；</p></li><li><p>在命令行界面输入<code>nvcc --version</code>命令，此时如果界面输出cuda的版本信息，则表名cuda安装成功。</p></li></ol><h2 id="安装cuDNN"><a href="#安装cuDNN" class="headerlink" title="安装cuDNN"></a>安装cuDNN</h2><ol><li><p>官网下载cudnn文件，下载需要先注册Nvidia开发者账号；</p></li><li><p>选择cuDNN v5.1 -&gt; cuDNN v5.1 Library for Linux进行下载；</p></li><li><p>解压下载文件，然后本地会生成一个cuda文件夹，再进行如下操作：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt;sudo cp cuda/include/cudnn.h /usr/local/cuda/include</div><div class="line">&gt;&gt;&gt;sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64</div></pre></td></tr></table></figure></li><li><p>进入<code>/usr/local/cuda/lib64</code>目录，执行如下操作：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; sudo rm -rf libcudnn.so libcudnn.so.5 #删除原动态链接文件</div><div class="line">&gt;&gt;&gt; sudo ln -s libcudnn.so.5.1.5 libcudnn.so.5  #重新生成软链接</div><div class="line">&gt;&gt;&gt; sudo ln -s libcudnn.so.5 libcudnn.so #重新生成软链接</div></pre></td></tr></table></figure></li><li><p>如果需要更换cuDNN版本，则替换掉原来的cudnn.h以及libcudnn*文件，再重新生成软链接即可。</p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;前提条件：已安装NVIDIA显卡驱动&lt;br&gt;安装环境：Ubuntu 16.04 LTS&lt;br&gt;Cuda版本：cuda 8.0&lt;br&gt;cuDNN版本：cuDNN 5.1&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;安装Cuda&quot;&gt;&lt;a href=&quot;#安装Cuda&quot; class=&quot;headerlink&quot; title=&quot;安装Cuda&quot;&gt;&lt;/a&gt;安装Cuda&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;下载cuda的run安装文件；
    
    </summary>
    
    
      <category term="软件安装" scheme="http://yoursite.com/tags/%E8%BD%AF%E4%BB%B6%E5%AE%89%E8%A3%85/"/>
    
  </entry>
  
  <entry>
    <title>Ubuntu下NVIDIA驱动安装指南</title>
    <link href="http://yoursite.com/2017/09/10/Ubuntu%E4%B8%8BNVIDIA%E9%A9%B1%E5%8A%A8%E5%AE%89%E8%A3%85%E6%8C%87%E5%8D%97/"/>
    <id>http://yoursite.com/2017/09/10/Ubuntu下NVIDIA驱动安装指南/</id>
    <published>2017-09-10T05:42:31.000Z</published>
    <updated>2017-09-10T05:55:21.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>安装环境：Ubuntu 16.04 LTS</p></blockquote><h2 id="第一步-禁止集成的nouveau驱动"><a href="#第一步-禁止集成的nouveau驱动" class="headerlink" title="第一步 禁止集成的nouveau驱动"></a>第一步 禁止集成的nouveau驱动</h2><p>由于Ubuntu自带了集成的第三方nouveau显卡驱动，如果不先去除，将无法安装Nvidia官方驱动。</p><a id="more"></a><ol><li><p>首先打开命令行界面，输入代码<code>lsmod | grep nouveau</code>，这时可以看出命令行有多行输出结果。</p></li><li><p>在<code>/etc/modprobe.d/</code>目录下新建文件名为blacklist-nouveau.conf的文件。</p></li><li><p>打开blacklist-nouveau.conf，并在其中添加如下代码：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">blacklist nouveau</div><div class="line">options nouveau modeset=0</div></pre></td></tr></table></figure></li><li><p>更新配置信息，输入命令<code>sudo update-initramfs -u</code>，此时需要重启电脑。</p></li><li><p>重启电脑后会发现电脑显示屏分辨率有明显变化，在命令行界面再次输入代码<code>lsmod | grep nouveau</code>，此时无任何输出，说明nouveau驱动已被禁止。</p></li></ol><h2 id="第二步-安装Nvidia官方驱动"><a href="#第二步-安装Nvidia官方驱动" class="headerlink" title="第二步 安装Nvidia官方驱动"></a>第二步 安装Nvidia官方驱动</h2><ol><li>在命令行界面输入命令<code>sudo service lightdm stop</code>以关闭X-window服务。</li><li>按住<code>Ctrl+Alt+F1</code>进入字符控制台界面。</li><li>输入用户名和密码登录系统，并进入显卡驱动所在的文件夹位置。</li><li>运行命令<code>sudo ./NVIDIAXXXXXXXX.run</code>完成安装。</li><li>安装完成后输入命令<code>sudo service lightdm start</code>开启X-window服务并进入图形界面，此时可以看到显示屏分辨率已调至最佳。</li><li>打开命令行工具，输入命令<code>nvidia-smi</code>，此时界面会有显卡信息输出，说明显卡驱动已安装成功。</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;安装环境：Ubuntu 16.04 LTS&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;第一步-禁止集成的nouveau驱动&quot;&gt;&lt;a href=&quot;#第一步-禁止集成的nouveau驱动&quot; class=&quot;headerlink&quot; title=&quot;第一步 禁止集成的nouveau驱动&quot;&gt;&lt;/a&gt;第一步 禁止集成的nouveau驱动&lt;/h2&gt;&lt;p&gt;由于Ubuntu自带了集成的第三方nouveau显卡驱动，如果不先去除，将无法安装Nvidia官方驱动。&lt;/p&gt;
    
    </summary>
    
    
      <category term="软件安装" scheme="http://yoursite.com/tags/%E8%BD%AF%E4%BB%B6%E5%AE%89%E8%A3%85/"/>
    
  </entry>
  
</feed>
