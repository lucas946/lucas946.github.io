---
title: 强化学习之蒙特卡洛方法
date: 2017-10-17 19:07:34
tags: 强化方法
mathjax: true
---
>  本文主要介绍强化学习中涉及的蒙特卡洛方法，该方法是强化学习求解免模型（model-free）方法中的重要组成部分，主要分为同策略（on-policy）和异策略（off-policy）两个分支。另外，在具体求解Q函数的时候，还分为first-visit和all-visit两种计算技巧。

<!--more-->
## 背景介绍

1. **什么是蒙特卡洛方法？**简单来说，蒙特卡洛方法就是利用统计学的思想来对Q函数进行求解，采用**平均累积奖赏作为期望累积奖赏的近似**。这样一来，就避免了大量的数值计算过程，并且还能够解决环境模型未知的情况。
2. 蒙特卡洛方法同样具有其局限性，它只能够解决有限状态下的强化学习任务，即每一次episode都有一个终止状态。
3. 值得注意的是，蒙特卡洛方法**只涉及到Q函数的求解**，并不涉及V函数的求解。这是因为通过Q函数我们可以很快得到最优策略$\pi$，而又因为此时环境未知，无法从V函数快速得到Q函数，所以此时对V函数求解意义不大。

## 方法描述

在模型未知的情况下，从初始状态出发，使用某种策略（通常初始策略是$\pi(a|s)=\frac{1}{A(s)}$）进行采样，执行该策略获得轨迹
$$
<x_0, a_0, r_1, x_1, a_1, r_2, \ldots, x_{T-1}, a_{T-1}, r_T, x_T>
$$
然后，对轨迹中出现的每一对状态-动作，记录其后的奖赏之和，作为该状态-动作对的一次累积奖赏采样值。多次采样得到多条轨迹后，将每个状态-动作对的累积奖赏值进行平均，即得到状态-动作值函数（Q函数）的估计。

##同策略方法

可以看出，要想得到Q函数的良好近似，就需要多次采样以得到不同的轨迹，以期遍历所有的**状态空间和动作空间**。同时，在实际操作中，可能我们每次迭代得到的策略是确定性的，即$\pi(a|s)=1$，或$\pi=arg \max_aQ(s,a)$，这样带来的后果就是我们无法遍历所有的状态空间和动作空间，面临“仅利用”的困境。为了跳出这一困境，就需要制定一种新的策略，使得所有动作都有可能被选中，这种新策略的数学定义如下：
$$
\pi^{\varepsilon}(x)=
\begin{cases}
    \pi(s) &\text{以概率}1-\varepsilon \\
    \text{A中以均匀概率选取的动作} &\text{以概率}\varepsilon
\end{cases}
$$
其中，$\pi(s)$被称为**原始策略**，即算法每次迭代后得出的策略，也是最后所要求的策略，$\pi^{\varepsilon}(s)$被称为**$\varepsilon$贪心策略**。从新策略的数学定义可以看出，当前最优动作被选中的概率是$1-\varepsilon+\frac{\varepsilon}{|A|}$，而每个非最优的动作被选中的概率是$\frac{\varepsilon}{|A|}$。 

![通策略蒙特卡洛算法](http://orjn2q9xs.bkt.clouddn.com/On-Policy%20MC.png)

同策略蒙特卡洛学习算法生成的是$\varepsilon$贪心策略，而我们要求的是原始策略，能否在策略评估的时候使用$\varepsilon$贪心策略，而在策略改进的使用使用原始策略呢？答案是肯定的，这就导致了异策略算法的发明。

## 异策略方法

### 重要性采样

函数$f(x)$在概率分布$p$下的期望可表达为
$$
\mathbb{E}[f]=\int_x p(x)f(x)dx
$$
可通过从概率分布$p$上的采样${x_1,x_2,\ldots,x_m}$来估计$f$的期望，即
$$
\hat{\mathbb{E}}[f]=\frac{1}{m} \sum_{i=1}^{m} f(x_i)
$$
这时候引入另一个分布$q$，则函数$f$在概率分布$p$下的期望也可等价地写为
$$
\mathbb{E}[f]=\int_x q(x)\frac{p(x)}{q(x)}f(x)dx
$$
通过在$q$上的采样${x_1^{'}, x_2^{'}, \ldots, x_m^{'}}$可估计$f$的期望如下：
$$
\hat{\mathbb{E}}[f]=\frac{1}{m} \sum_{i=1}^{m} \frac{p(x_i^{'})}{q(x_i^{'})} f(x_i^{'})
$$
这样的基于一个分布的采样来估计另一个分布下的期望称为**重要性采样**。

### 算法推导

使用策略$\pi$的采样轨迹来评估策略$\pi$，实际上就是用累积奖赏估计期望
$$
Q(x,a)=\frac{1}{m} \sum_{i=1}^{m}R_i
$$
由上述的重要性采样定理可知，我们可以通过策略$\pi^{'}$的采样轨迹来评估策略$\pi$，则仅需对累积奖赏加权
$$
Q(x,a)=\frac{1}{m} \sum_{i=1}^{m} \frac{P^{\pi}_i}{P^{\pi^{'}}_i} R_i
$$
其中，$P^{\pi}_i$和$P^{\pi^{'}}_i$分别表示两个策略产生第$i$条轨迹的概率。对于一条给定的轨迹$<x_0,a_0,r_1,\ldots, x_{T-1}, a_{T-1}, r_T, x_T >$，策略$\pi$产生该轨迹的概率为
$$
P^{\pi}=\prod_{i=0}^{T-1} \pi(x_i,a_i)P^{a_i}_{x_i \rightarrow x_{i+1}}
$$
由此可以计算得出$P^{\pi}_i$和$P^{\pi^{'}}_i$的比值为
$$
\frac{P^{\pi}}{P^{\pi^{'}}}=\prod_{i=0}^{T-1}\frac{\pi(x_i,a_i)}{\pi^{'}(x_i,a_i)}
$$
异策略蒙特卡洛算法就是在策略评估的时候使用的是$\varepsilon$贪心策略，而在策略改进的时候使用的是原始策略，这样最后算法得到的就是原始策略，符合我们的要求。其算法流程如下所示：

![异策略蒙特卡洛学习](http://orjn2q9xs.bkt.clouddn.com/Off-Policy%20MC.png)


