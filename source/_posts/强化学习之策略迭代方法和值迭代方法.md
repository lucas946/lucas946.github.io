---
title: 强化学习之策略迭代方法和值迭代方法
date: 2017-10-16 17:18:10
tags: 强化学习
mathjax: true
---
在马尔科夫决策过程$(S, A, P, R,\gamma )$所有参数全部知道的情况下，要想求解Bellman最优化方程，需要运用到动态规划（Dynamic Programming）的思想，有两种方法，一种被叫作基于策略的迭代，另一种是基于值函数的迭代。下面分别阐述之。
<!--more-->
## 策略迭代（Policy Iteration）

基于策略的迭代基本上可以看做是对**贝尔曼期望方程**不断迭代的一个求解过程，贝尔曼期望方程如下所示：
$$
v_{\pi}(s)=\sum_{a\in A} \pi(a|s)[R_s^a+\gamma\sum_{s^{'}\in S}P_{ss^{'}}^a v_{\pi}(s^{'})]
$$
其伪代码如下所示：

1. Initialization

   $v_{\pi}(s) \in \mathbb{R}$ and $\pi(a|s)\in [0,1], a\in A$ arbitrarily for all $s\in S$

2. Policy Evaluation

   Repeat

   ​	$\varDelta \leftarrow 0$

   ​	For each $s\in S$

   ​		$v \leftarrow v_{\pi}(s)$

   ​		$v_{\pi}(s) \leftarrow \sum_{a\in A} \pi(a|s)[R_s^a+\gamma\sum_{s^{'}\in S}P_{ss^{'}}^a v_{\pi}(s^{'})]$

   ​		$\varDelta \leftarrow max(\varDelta, |v-v_{\pi}(s)|)$

   Until $\varDelta < \theta$ ($\theta$ is a small positive number)

3. Policy Improvement

   policy-stable $\leftarrow$ true

   For each $s\in S$

   ​	$a\leftarrow \pi(s)$

   ​	$\pi(s)\leftarrow arg \max_{a\in A} R_s^a+\gamma\sum_{s^{'}\in S}P_{ss^{'}}^a v_{\pi}(s^{'})$

   ​	If $a \neq \pi(s)$, then policy-stable$\leftarrow$false

   If policy-stable, then stop and return $v_{\pi}$ and $\pi$; else go to 2

从上述伪代码可以看出，基于策略的迭代分为两个步骤，一个是策略评估，一个是策略改进，并且最后该程序一定会收敛，并且收敛的时候，$v_{\pi}=v_{*}, \pi = \pi_{*}$。

## 值迭代（Value Iteration）

值迭代则是使用**贝尔曼最优化方程**而不是贝尔曼期望方程得到的，贝尔曼最优化方程如下所示：
$$
v_{*}(s)=\max_{a\in A}R_s^a+\gamma \sum_{s^{'}\in S}P_{ss^{'}}^a v_{*}(s^{'})
$$
其伪代码如下所示：

Initialize array $v_{*}(s)$ arbitrarily (e.g., v(s) = 0 for all $s\in S$)

Repeat

​	$\varDelta \leftarrow 0$

​	For each $s\in S$

​		$v \leftarrow v_{*}(s)$	

​		$v_{*}(s)\leftarrow \max_{a\in A} R_s^a+\gamma\sum_{s^{'}\in S}P_{ss^{'}}^a v_{*}(s^{'})$	 

​		$\varDelta \leftarrow max(\varDelta,|v-v_{*}(s)|)$

​	until $\varDelta < \theta$ ($\theta$ is a small positive number)

Output a deterministic policy, $\pi$, such that
$$
\pi(s)= arg \max_{a\in A} R_s^a+\gamma\sum_{s^{'}\in S}P_{ss^{'}}^a v_{*}(s^{'})
$$

可以看出，基于值函数的迭代并不需要提前知道策略是什么，其算法流程相比于基于策略的迭代更为简洁和紧凑。

## 结论

基于策略的迭代和基于值函数的迭代各有优势，并且适应于各自的领域。基于策略的迭代输出的是采取每种动作的概率值，而基于值函数的迭代输出的是一个确定的动作，所以，基于策略的迭代适合于连续动作空间，而基于值函数的迭代适合于离散动作空间。
